<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="MiDiff: MIDI-Conditioned Diffusion Models for Drum Audio Enhancement">
  <meta property="og:title" content="MiDiff: MIDI-Conditioned Diffusion Models for Drum Audio Enhancement"/>
  <meta property="og:description" content="MiDiff: MIDI-Conditioned Diffusion Models for Drum Audio Enhancement"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/overview.PNG" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="MiDiff: MIDI-Conditioned Diffusion Models for Drum Audio Enhancement">
  <meta name="twitter:description" content="MiDiff: MIDI-Conditioned Diffusion Models for Drum Audio Enhancement">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/overview.PNG">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Audio Denoising, Diffusion Models, CTC Loss">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MiDiff: MIDI-Conditioned Diffusion Models for Drum Audio Enhancement</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <style>
    /* Force scrollbars to always be visible on spectrogram containers */
    .spectrogram-scroll-container::-webkit-scrollbar {
      height: 12px;
    }
    .spectrogram-scroll-container::-webkit-scrollbar-track {
      background: #f1f1f1;
      border-radius: 6px;
    }
    .spectrogram-scroll-container::-webkit-scrollbar-thumb {
      background: #888;
      border-radius: 6px;
    }
    .spectrogram-scroll-container::-webkit-scrollbar-thumb:hover {
      background: #555;
    }
    /* For Firefox */
    .spectrogram-scroll-container {
      scrollbar-width: auto;
      scrollbar-color: #888 #f1f1f1;
    }
    /* Epoch card sizing */
    .epoch-card {
      width: 210px !important;
      min-width: 210px !important;
    }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <!-- SeeWav for audio waveform visualization -->
  <script src="https://unpkg.com/seewav@1.0.0/dist/seewav.min.js"></script>
  <!-- Chart.js for interactive graphs -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <!-- MIDI Player -->
  <script src="https://cdn.jsdelivr.net/combine/npm/tone@14.7.58,npm/@magenta/music@1.23.1/es6/core.js,npm/focus-visible@5,npm/html-midi-player@1.5.0"></script>
  <!-- MathJax Configuration -->
  <script>
  window.MathJax = {
    tex: {
      inlineMath: [['\\(', '\\)']],
      displayMath: [['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
  </script>
  <!-- MathJax Library -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <!-- MIDI parsing library -->
  <script src="https://cdn.jsdelivr.net/npm/midi-parser-js@4.0.4/dist/midi-parser.min.js"></script>
  <!-- Backup MIDI parser -->
  <script src="https://unpkg.com/midi-file@1.2.4/midi-file.js"></script>
  <!-- Tone.js for audio synthesis -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/tone/14.7.58/Tone.js"></script>
  <!-- TensorFlow.js for Magenta -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/tensorflow/1.2.8/tf.min.js"></script>
  <!-- Magenta.js core for Player -->
  <script src="https://cdn.jsdelivr.net/npm/@magenta/music@1.9.0"></script>
  <!-- D3.js for visualizations -->
  <script src="https://cdn.jsdelivr.net/npm/d3@5.7.0/dist/d3.min.js"></script>
  <!-- Web Audio API FFT implementation -->

</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MiDiff: MIDI-Conditioned Diffusion Models for Drum Audio Enhancement</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="#" target="_blank">Elad Aspis</a><sup>1</sup></span>,
                <span class="author-block"><a href="#" target="_blank">Sharon Gannot</a><sup>1</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Bar Ilan University
                      <!-- Conferance name and year</span> -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="#" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="#" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-widescreen">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present MIDI-conditioned diffusion models for drum audio enhancement that leverage symbolic musical information to guide the denoising process. Our method extends SGMSE by incorporating MIDI conditioning through Feature-wise Linear Modulation (FiLM) layers within the NCSNPP U-Net architecture. The MIDI encoder processes piano roll representations to generate conditioning signals that modulate the diffusion model's behavior, enabling musically-informed enhancement decisions. Evaluation on drum audio datasets with (clean, noisy, MIDI) triplets demonstrates improved Frechet VGGish Scores and superior perceptual quality with better preservation of rhythmic structure compared to baseline models.
          </p>
        </div>
      </div>
    </div>

    <!-- Architecture Section with Text and Figures -->
    <div class="columns is-centered" style="margin-top: 3rem;">
      <div class="column is-four-fifths">
        <!-- First Row: Text on Left, MIDI Encoder on Right -->
        <div class="columns is-centered">
          <div class="column is-6">
            <div class="content has-text-justified">
              <h3 class="title is-4">MIDI-Conditioned Architecture</h3>
              <p>
                Our approach builds upon SGMSE (Score-based Generative Models for Speech Enhancement) as the baseline framework, extending it with MIDI conditioning for drum audio enhancement. We utilize the MIDI information as a conditioning signal within the NCSNPP (Noise Conditional Score Network++) U-Net architecture, enabling the model to leverage symbolic musical context during the enhancement process.
              </p>
              <p>
                The core architectural modification involves integrating MIDI conditioning into the NCSNPP residual blocks. Specifically, we add an additional FiLM (Feature-wise Linear Modulation) layer to each residual block, enabling the network to dynamically modulate its behavior based on the MIDI input. The MIDI conditioning is processed through a custom encoder that takes MIDI roll representations and transforms them into continuous embeddings \(\mathbf{h}_{\text{MIDI}} \in \mathbb{R}^d\).
              </p>
              <p>
                The MIDI encoder processes piano roll representations of the drum patterns, capturing note onsets and velocities. This encoder architecture consists of convolutional layers followed by temporal processing stages that extract both local rhythmic patterns and global musical structure. The resulting MIDI embeddings are then used to generate FiLM parameters (\(\gamma\) and \(\beta\)) that modulate the U-Net features at multiple scales, allowing the diffusion model to align audio enhancement with the underlying musical intent and drum performance characteristics.
              </p>
            </div>
          </div>
          <div class="column is-1">
          </div>
          <div class="column is-4" style="margin-top: 4rem;">
            <figure>
              <img src="static/figures/MidiConditioned.png" alt="Residual Block with MIDI Conditioning via FiLM Layers" style="width: 100%;">
              <figcaption class="" style="text-align: center;">
                <strong>Figure 1:</strong> Residual Block with MIDI Conditioning via FiLM Layers
              </figcaption>
            </figure>
          </div>
        </div>
        
        <!-- Second Row: FiLM Implementation Below -->
        <div class="columns is-centered" style="margin-top: 3rem;">
          <div class="column is-6">
            <div class="content has-text-justified">
              <h3 class="title is-4">Feature-wise Linear Modulation (FiLM)</h3>
              <p>
                FiLM is a powerful conditioning mechanism that modulates the activations of a neural network through feature-wise affine transformations. Given input features \(\mathbf{x}\) and conditioning information (in our case, MIDI embeddings), FiLM applies element-wise scaling and shifting operations:
              </p>
              <p style="text-align: center; background: #f8f9fa; border-left: 4px solid #007bff; padding: 1.5rem; margin: 2rem 0; font-size: 1.15rem;">
                \[\text{FiLM}(\mathbf{x} \mid \gamma, \beta) = \gamma \odot \mathbf{x} + \beta\]
              </p>
              <p>
                where \(\gamma\) and \(\beta\) are learned modulation parameters derived from the MIDI conditioning signal through neural network layers. The scale parameter \(\gamma\) controls the magnitude of features, while the shift parameter \(\beta\) provides an additive bias. This adaptive modulation allows the diffusion model to dynamically adjust its behavior based on the MIDI input.
              </p>
              <p>
                In our architecture, the FiLM parameters are generated from the MIDI encoder output:
              </p>
              <p style="text-align: center; background: #f8f9fa; border-left: 4px solid #28a745; padding: 1.5rem; margin: 2rem 0; font-size: 1.15rem;">
                \[\gamma = f_{\gamma}(\mathbf{h}_{\text{MIDI}}), \quad \beta = f_{\beta}(\mathbf{h}_{\text{MIDI}})\]
              </p>
              <p>
                where \(\mathbf{h}_{\text{MIDI}}\) represents the encoded MIDI features, and \(f_{\gamma}\), \(f_{\beta}\) are learned projection functions. This conditioning mechanism is applied at multiple layers throughout the diffusion U-Net, enabling the network to incorporate MIDI information effectively throughout the denoising process.
              </p>
            </div>
          </div>
          <div class="column is-2">
          </div>

          <div class="column is-3" style="margin-top: 5rem;">
            <figure>
              <img src="static/figures/FiLM_implementation.png" alt="FiLM Implementation" style="width: 100%;">
              <figcaption>
                <strong>Figure 2:</strong> Feature-wise Linear Modulation (FiLM) Implementation
              </figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>

    <!-- Dataset Section -->
    <div class="columns is-centered" style="margin-top: 3rem;">
      <div class="column is-four-fifths">
        <h3 class="title is-3">Dataset</h3>
        <div class="content has-text-justified">
          <p>
            Our training dataset consists of triplets containing (clean, noisy, MIDI) samples for drum audio enhancement. The clean audio serves as ground truth, noisy versions simulate realistic degradation scenarios, and MIDI data provides symbolic musical context including note onsets, velocities, and timing information.
          </p>
          <p>
            This multimodal structure enables the model to learn relationships between symbolic musical information and audio quality, allowing for musically-informed enhancement decisions. The MIDI component serves as a strong conditioning signal, providing explicit knowledge about drum patterns and timing for the enhancement process.
          </p>
            <p>The following samples provide a direct perceptual comparison. The "Clean Original" and "Noisy Input" are provided as references.</p>
        </div>
      </div>
    </div>

    <div class="content">
    </div>
    
    <!-- Clean, Noisy, and MIDI Reference in One Line -->
    <div class="columns is-centered" style="margin-bottom: 30px;">
      <!-- Clean Audio Reference -->
      <div class="column is-4">
        <div class="box" style="background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border: 2px solid #dee2e6; border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.1);">
          <div class="level is-mobile" style="margin-bottom: 15px;">
            <div class="level-left">
              <div class="level-item">
                <h4 class="title is-6" style="color: #2c3e50; margin-bottom: 0;">
                  <span class="icon" style="color: #27ae60;">
                    <i class="fas fa-music"></i>
                  </span>
                  Clean Reference Audio
                </h4>
              </div>
            </div>
          </div>
          
          <!-- Compact Spectrogram visualization -->
          <div class="spectrogram-scroll-container" style="margin-bottom: 15px; display: flex; justify-content: flex-start; border: 2px solid #dee2e6; border-radius: 8px; overflow-x: scroll; overflow-y: hidden; white-space: nowrap; box-shadow: 0 4px 12px rgba(0,0,0,0.3); height: 270px; cursor: grab;">
            <img src="static/images/spec_clean.png" alt="Clean Audio Spectrogram" style="height: 265px; width: auto; display: inline-block; max-width: none;">
          </div>
          
          <div class="field">
            <div class="control">
              <audio id="audio-clean" controls style="width: 100%; height: 40px;">
                <source src="static/clean_10_soul-groove10_102_4-4_bluebird.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
          </div>
        </div>
      </div>

      <!-- Noisy Audio Reference -->
      <div class="column is-4">
        <div class="box" style="background: linear-gradient(135deg, #fff3cd 0%, #ffeaa7 100%); border: 2px solid #ffc107; border-radius: 12px; box-shadow: 0 8px 25px rgba(255,193,7,0.2);">
          <div class="level is-mobile" style="margin-bottom: 15px;">
            <div class="level-left">
              <div class="level-item">
                <h4 class="title is-6" style="color: #856404; margin-bottom: 0;">
                  <span class="icon" style="color: #f39c12;">
                    <i class="fas fa-music"></i>
                  </span>
                  Noisy Input Audio
                </h4>
              </div>
            </div>
          </div>
          
          <!-- Compact Spectrogram visualization -->
          <div class="spectrogram-scroll-container" style="margin-bottom: 15px; display: flex; justify-content: flex-start; border: 2px solid #ffc107; border-radius: 8px; overflow-x: scroll; overflow-y: hidden; white-space: nowrap; box-shadow: 0 4px 12px rgba(0,0,0,0.3); height: 270px; cursor: grab;">
            <img src="static/images/spec_noisy.png" alt="Noisy Audio Spectrogram" style="height: 265px; width: auto; display: inline-block; max-width: none;">
          </div>
          
          <div class="field">
            <div class="control">
              <audio id="audio-noisy" controls style="width: 100%; height: 40px;">
                <source src="static/noisy_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
          </div>
        </div>
      </div>

      <!-- MIDI Input File -->
      <div class="column is-4">
        <div class="box" style="background: linear-gradient(135deg, #e8f5e8 0%, #d4edda 100%); border: 2px solid #28a745; border-radius: 12px; box-shadow: 0 8px 25px rgba(40,167,69,0.2);">
          <div class="level is-mobile" style="margin-bottom: 15px;">
            <div class="level-left">
              <div class="level-item">
                <h4 class="title is-6" style="color: #155724; margin-bottom: 0;">
                  <span class="icon" style="color: #28a745;">
                    <i class="fas fa-file-audio"></i>
                  </span>
                  Condition - MIDI Input File
                </h4>
              </div>
            </div>
          </div>
          
          <!-- MIDI Piano Roll Visualization (Canvas) -->
          <div style="margin-bottom: 15px; display: flex; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); background: #E8E8E8; overflow: hidden;">
            <!-- Drum labels on the left -->
            <canvas id="drum-labels-canvas" style="display: block; height: 265px; width: 85px; border-right: 1px solid rgba(0, 0, 0, 0.1);"></canvas>
            <!-- Scrollable piano roll -->
            <div id="piano-roll-container" style="overflow-x: auto; flex: 1; position: relative;">
              <canvas id="piano-roll-canvas" style="display: block; height: 265px;"></canvas>
            </div>
          </div>
          
          <!-- MIDI Player -->
          <div class="field">
            <div class="control">
              <midi-player
                id="midi-player-main"
                src="static/10_soul-groove10_102_beat_4-4.mid"
                sound-font
                visualizer="#midi-visualizer"
                style="width: 100%; border-radius: 8px; display: block; margin: 0; padding: 0;">
              </midi-player>
            </div>
          </div>
          
          <script src="https://cdn.jsdelivr.net/npm/@tonejs/midi@2.0.28/build/Midi.js"></script>
          <script>
            // Real-time MIDI Piano Roll Visualization
            (async function() {
              const player = document.getElementById('midi-player-main');
              const container = document.getElementById('piano-roll-container');
              const canvas = document.getElementById('piano-roll-canvas');
              const ctx = canvas.getContext('2d');
              const labelsCanvas = document.getElementById('drum-labels-canvas');
              const labelsCtx = labelsCanvas.getContext('2d');
              
              // Custom drum map for this specific MIDI file (9 drums)
              const drumNames = {
                36: 'Bass',
                38: 'Snare',
                42: 'HH Close',
                46: 'HH Open',
                43: 'Floor Tom',
                47: 'Mid Tom',
                48: 'High Tom',
                49: 'Crash',
                51: 'Ride'
              };
              
              let midiData = null;
              let allNotes = [];
              let duration = 0;
              // Use only the actual pitches present in the MIDI (9 drums)
              const usedPitches = [36, 38, 42, 46, 43, 47, 48, 49, 51]; // Bass, Snare, HH Close, HH Open, Floor Tom, Mid Tom, High Tom, Crash, Ride
              const pixelsPerSecond = 50; // Scale: 50px per second (more notes visible)
              
              // Set label canvas size
              labelsCanvas.width = 85;
              labelsCanvas.height = 265; // Balanced height for visualization
              
              // Load and parse MIDI file
              try {
                const response = await fetch('static/10_soul-groove10_102_beat_4-4.mid');
                const arrayBuffer = await response.arrayBuffer();
                midiData = await Midi.fromUrl('static/10_soul-groove10_102_beat_4-4.mid');
                
                // Extract all notes from all tracks
                midiData.tracks.forEach(track => {
                  track.notes.forEach(note => {
                    allNotes.push({
                      pitch: note.midi,
                      start: note.time,
                      end: note.time + note.duration,
                      velocity: note.velocity
                    });
                  });
                });
                
                duration = midiData.duration;
                
                // Debug: Show all unique pitches
                const uniquePitches = [...new Set(allNotes.map(n => n.pitch))].sort((a, b) => a - b);
                console.log(`Loaded MIDI: ${allNotes.length} notes, duration: ${duration}s`);
                console.log(`Unique pitches in MIDI:`, uniquePitches);
                console.log(`Using pitch mapping:`, usedPitches);
                
                // Set canvas size
                const canvasWidth = duration * pixelsPerSecond;
                canvas.width = canvasWidth;
                canvas.height = 265; // Balanced height for visualization
                
                // Initial draw
                drawPianoRoll(0);
                drawDrumLabels();
                
                // Animation loop for cursor and scrolling
                function animate() {
                  if (player.playing) {
                    const currentTime = player.currentTime || 0;
                    drawPianoRoll(currentTime);
                    
                    // Auto-scroll to keep cursor centered
                    const cursorX = currentTime * pixelsPerSecond;
                    const containerWidth = container.offsetWidth;
                    container.scrollLeft = cursorX - (containerWidth / 2);
                  }
                  requestAnimationFrame(animate);
                }
                animate();
                
              } catch (error) {
                console.error('Error loading MIDI:', error);
                ctx.fillStyle = '#28a745';
                ctx.font = '16px sans-serif';
                ctx.fillText('Error loading MIDI file', 10, 150);
              }
              
              function drawPianoRoll(currentTime) {
                // Clear canvas with light gray background
                ctx.fillStyle = '#E8E8E8';
                ctx.fillRect(0, 0, canvas.width, canvas.height);
                
                const numPitches = usedPitches.length; // 6 pitches
                const pitchHeight = canvas.height / numPitches;
                
                // Draw alternating row backgrounds for better readability
                usedPitches.forEach((pitch, index) => {
                  if (index % 2 === 0) {
                    const y = canvas.height - ((index + 1) * pitchHeight);
                    ctx.fillStyle = 'rgba(0, 0, 0, 0.03)';
                    ctx.fillRect(0, y, canvas.width, pitchHeight);
                  }
                });
                
                // Draw enhanced grid lines
                ctx.strokeStyle = 'rgba(40, 167, 69, 0.15)';
                ctx.lineWidth = 0.5;
                
                // Vertical grid lines (every second) with stronger lines every 4 seconds
                for (let t = 0; t <= duration; t += 1) {
                  const x = t * pixelsPerSecond;
                  if (t % 4 === 0) {
                    ctx.strokeStyle = 'rgba(40, 167, 69, 0.25)';
                    ctx.lineWidth = 1;
                  } else {
                    ctx.strokeStyle = 'rgba(40, 167, 69, 0.1)';
                    ctx.lineWidth = 0.5;
                  }
                  ctx.beginPath();
                  ctx.moveTo(x, 0);
                  ctx.lineTo(x, canvas.height);
                  ctx.stroke();
                }
                
                // Horizontal grid lines between each drum
                ctx.strokeStyle = 'rgba(40, 167, 69, 0.1)';
                ctx.lineWidth = 0.5;
                for (let i = 0; i <= numPitches; i++) {
                  const y = canvas.height - (i * pitchHeight);
                  ctx.beginPath();
                  ctx.moveTo(0, y);
                  ctx.lineTo(canvas.width, y);
                  ctx.stroke();
                }
                
                // Draw all notes with drum-specific color palette
                allNotes.forEach(note => {
                  // Find the index of this pitch in our used pitches
                  const pitchIndex = usedPitches.indexOf(note.pitch);
                  if (pitchIndex === -1) return; // Skip notes not in our list
                  
                  const x = note.start * pixelsPerSecond;
                  const width = Math.max((note.end - note.start) * pixelsPerSecond, 2);
                  const y = canvas.height - ((pitchIndex + 1) * pitchHeight);
                  const height = pitchHeight * 0.85;
                  
                  // Drum-specific color palette - soft pastel colors
                  const drumColors = {
                    36: { r: 126, g: 200, b: 227, name: 'Sky Blue' },         // Bass/Kick
                    38: { r: 255, g: 166, b: 158, name: 'Salmon' },           // Snare
                    42: { r: 162, g: 210, b: 255, name: 'Aqua' },             // Hi-Hat Closed
                    46: { r: 184, g: 242, b: 230, name: 'Mint' },             // Hi-Hat Open
                    43: { r: 160, g: 206, b: 217, name: 'Teal' },             // Floor Tom (Low Tom)
                    47: { r: 224, g: 187, b: 228, name: 'Lilac' },            // Mid Tom
                    48: { r: 255, g: 218, b: 193, name: 'Peach' },            // High Tom
                    49: { r: 253, g: 253, b: 150, name: 'Light Yellow' },     // Crash
                    51: { r: 203, g: 170, b: 203, name: 'Lavender' }          // Ride
                  };
                  
                  const velocity = note.velocity;
                  const isPlaying = currentTime >= note.start && currentTime <= note.end;
                  
                  // Get base color for this drum
                  const baseColor = drumColors[note.pitch] || { r: 100, g: 100, b: 100 };
                  
                  // Adjust brightness and alpha based on velocity
                  const velocityFactor = 0.4 + (velocity * 0.6); // Range from 40% to 100%
                  const r = Math.floor(baseColor.r * velocityFactor);
                  const g = Math.floor(baseColor.g * velocityFactor);
                  const b = Math.floor(baseColor.b * velocityFactor);
                  const alpha = 0.6 + (velocity * 0.4); // Range from 0.6 to 1.0
                  
                  if (isPlaying) {
                    // Highlight currently playing notes with intense glow
                    const glowIntensity = 10 + velocity * 15; // Stronger glow for louder notes
                    ctx.shadowBlur = glowIntensity;
                    ctx.shadowColor = `rgba(${baseColor.r}, ${baseColor.g}, ${baseColor.b}, ${Math.min(alpha + 0.4, 1)})`;
                    
                    // Brighter color when playing (use full base color)
                    const playR = Math.min(255, baseColor.r + 40);
                    const playG = Math.min(255, baseColor.g + 40);
                    const playB = Math.min(255, baseColor.b + 40);
                    
                    const playGradient = ctx.createLinearGradient(x, y, x, y + height);
                    playGradient.addColorStop(0, `rgba(${playR}, ${playG}, ${playB}, ${Math.min(alpha + 0.3, 1)})`);
                    playGradient.addColorStop(1, `rgba(${baseColor.r}, ${baseColor.g}, ${baseColor.b}, ${alpha})`);
                    ctx.fillStyle = playGradient;
                    ctx.strokeStyle = `rgba(${playR}, ${playG}, ${playB}, 1)`;
                    ctx.lineWidth = 1.5 + velocity;
                  } else {
                    ctx.shadowBlur = 0;
                    
                    // Create velocity-based gradient for notes
                    const noteGradient = ctx.createLinearGradient(x, y, x, y + height);
                    noteGradient.addColorStop(0, `rgba(${r}, ${g}, ${b}, ${alpha})`);
                    noteGradient.addColorStop(1, `rgba(${Math.floor(r * 0.7)}, ${Math.floor(g * 0.7)}, ${Math.floor(b * 0.7)}, ${alpha * 0.8})`);
                    ctx.fillStyle = noteGradient;
                    ctx.strokeStyle = `rgba(${r}, ${g}, ${b}, ${alpha * 0.9})`;
                    ctx.lineWidth = 0.5 + velocity * 0.5;
                  }
                  
                  // Draw rounded rectangle for notes
                  const radius = Math.min(2, height / 4);
                  ctx.beginPath();
                  ctx.roundRect(x, y, width, height, radius);
                  ctx.fill();
                  ctx.stroke();
                  ctx.shadowBlur = 0;
                });
                
                // Draw playback cursor with enhanced styling
                if (player.playing) {
                  const cursorX = currentTime * pixelsPerSecond;
                  
                  // Draw glow effect
                  ctx.shadowBlur = 10;
                  ctx.shadowColor = 'rgba(255, 100, 100, 0.6)';
                  
                  // Draw cursor line with gradient
                  const cursorGradient = ctx.createLinearGradient(0, 0, 0, canvas.height);
                  cursorGradient.addColorStop(0, 'rgba(255, 50, 50, 0.9)');
                  cursorGradient.addColorStop(0.5, 'rgba(255, 80, 80, 1)');
                  cursorGradient.addColorStop(1, 'rgba(255, 50, 50, 0.9)');
                  ctx.strokeStyle = cursorGradient;
                  ctx.lineWidth = 3;
                  ctx.beginPath();
                  ctx.moveTo(cursorX, 0);
                  ctx.lineTo(cursorX, canvas.height);
                  ctx.stroke();
                  ctx.shadowBlur = 0;
                  
                  // Draw time label with background
                  const timeText = `${currentTime.toFixed(1)}s`;
                  ctx.font = 'bold 11px sans-serif';
                  const textWidth = ctx.measureText(timeText).width;
                  
                  // Background for time label
                  ctx.fillStyle = 'rgba(255, 50, 50, 0.9)';
                  ctx.fillRect(cursorX + 3, 8, textWidth + 8, 16);
                  
                  // Time text
                  ctx.fillStyle = '#ffffff';
                  ctx.fillText(timeText, cursorX + 7, 19);
                }
              }
              
              function drawDrumLabels() {
                // Clear labels canvas with light gray background
                labelsCtx.fillStyle = '#E8E8E8';
                labelsCtx.fillRect(0, 0, labelsCanvas.width, labelsCanvas.height);
                
                const numPitches = usedPitches.length; // 6 pitches
                const pitchHeight = labelsCanvas.height / numPitches;
                
                // Draw alternating row backgrounds
                usedPitches.forEach((pitch, index) => {
                  if (index % 2 === 0) {
                    const y = labelsCanvas.height - ((index + 1) * pitchHeight);
                    labelsCtx.fillStyle = 'rgba(0, 0, 0, 0.03)';
                    labelsCtx.fillRect(0, y, labelsCanvas.width, pitchHeight);
                  }
                });
                
                labelsCtx.font = 'bold 10px sans-serif';
                labelsCtx.textAlign = 'right';
                labelsCtx.textBaseline = 'middle';
                
                usedPitches.forEach((pitch, index) => {
                  const y = labelsCanvas.height - ((index + 0.5) * pitchHeight);
                  
                  // Determine label text
                  let label = drumNames[pitch] || `${pitch}`;
                  
                  // Draw subtle horizontal line
                  labelsCtx.strokeStyle = 'rgba(40, 167, 69, 0.15)';
                  labelsCtx.lineWidth = 0.5;
                  labelsCtx.beginPath();
                  labelsCtx.moveTo(0, y);
                  labelsCtx.lineTo(labelsCanvas.width, y);
                  labelsCtx.stroke();
                  
                  // Draw label text with subtle shadow for better readability
                  labelsCtx.shadowBlur = 2;
                  labelsCtx.shadowColor = 'rgba(255, 255, 255, 0.5)';
                  labelsCtx.fillStyle = '#000000';
                  labelsCtx.fillText(label, labelsCanvas.width - 4, y);
                  labelsCtx.shadowBlur = 0;
                });
              }
            })();
          </script>
          
          <!-- <div class="field">
            <div class="control">
              <a href="static/10_soul-groove10_102_beat_4-4.mid" download class="button is-success is-fullwidth">
                <span class="icon">
                  <i class="fas fa-download"></i>
                </span>
                <span>Download MIDI</span>
              </a>
            </div>
          </div> -->
        </div>
      </div>
    </div>

    <!-- Results Section -->
    <div class="columns is-centered" style="margin-top: 3rem;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content">
          <p>To quantitatively evaluate performance, we calculated standard audio quality metrics. The Frechet VGGish Score measures the similarity between distributions of embeddings from the generated audio and the ground truth clean audio. <strong>A lower score indicates higher similarity to the clean audio.</strong></p>
        </div>
        
        <figure style="margin-top: 2rem;">
          <div style="position: relative; width: 100%; height: 400px; margin: 20px 0;">
            <div style="position: absolute; top: 10px; right: 10px; z-index: 10; background: white; padding: 8px 12px; border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); border: 1px solid #e0e0e0;">
              <label class="checkbox" style="margin: 0; cursor: pointer; display: flex; align-items: center; gap: 6px; font-size: 0.9rem; color: #4a4a4a;">
                <input type="checkbox" id="logScaleToggle" onchange="toggleLogScale()" checked style="cursor: pointer;">
                <span>Log Scale</span>
              </label>
            </div>
            <canvas id="frechetChart"></canvas>
          </div>
          <figcaption style="text-align: center;"><strong>Figure 3:</strong> Interactive Frechet VGGish Score for Baseline vs. Midi Conditioned models.</figcaption>
        </figure>
      </div>
    </div>

    <h2 class="title is-3" style="margin-top: 3rem;">Audio Samples</h2>

    <div class="columns is-centered">
      <div class="column is-full" style="max-width: 1400px; padding: 0 2rem;">
        <h3 class="title is-4" style="text-align: left; margin-bottom: 2rem;">Model Outputs Comparison</h3>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full" style="max-width: 1400px; padding: 0 2rem;">
        <!-- Baseline Model Section -->
        <div style="background: linear-gradient(135deg, #fff5f5 0%, #ffe9e9 100%); padding: 2rem; border-radius: 16px; margin-bottom: 2.5rem; box-shadow: 0 4px 20px rgba(255, 99, 132, 0.1); border: 2px solid #ffd4d4;">
          <h4 class="title is-5" style="margin-bottom: 0rem; color: #c92a2a; display: flex; align-items: center; gap: 10px;">
            <span style="background: #ff6b6b; width: 8px; height: 24px; border-radius: 4px; display: inline-block;"></span>
            Baseline Model
          </h4>
          <div style="display: flex; align-items: center; justify-content: center; gap: 20px; flex-wrap: wrap;">
            <!-- Epoch 0 -->
            <div class="epoch-card" style="background: white; border-radius: 12px; padding: 16px; width: 280px; min-width: 260px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.08); border: 1px solid #f0f0f0; transition: transform 0.2s, box-shadow 0.2s;">
              <div style="margin-bottom: 12px; font-weight: 600; color: #495057; font-size: 0.95rem; text-transform: uppercase; letter-spacing: 0.5px;">Epoch 0</div>
              <div class="spectrogram-scroll-container" style="border: 2px solid #e9ecef; border-radius: 8px; overflow-x: scroll; overflow-y: hidden; white-space: nowrap; margin-bottom: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); height: 94px; cursor: grab; max-width: 248px; -webkit-overflow-scrolling: touch;">
                <img src="static/images/baseline_epoch=0_10_soul-groove10_102_4-4_bluebird_v1_noisy.png" alt="Spectrogram Epoch 0" style="height: 90px; width: auto; display: inline-block; max-width: none;">
              </div>
              <audio controls style="width: 100%; height: 36px;">
                <source src="static/audio/baseline/epoch=0_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav" type="audio/wav">
              </audio>
            </div>
            <div style="display: flex; align-items: center; justify-content: center; width: 60px;">
              <span style="font-size: 3rem; background: linear-gradient(135deg, #ff6b6b 0%, #ff8787 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; font-weight: 600; filter: drop-shadow(2px 2px 4px rgba(255, 107, 107, 0.3)); animation: pulse-arrow 2s ease-in-out infinite;">→</span>
            </div>
            <!-- Epoch 10 -->
            <div class="epoch-card" style="background: white; border-radius: 12px; padding: 16px; width: 280px; min-width: 260px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.08); border: 1px solid #f0f0f0; transition: transform 0.2s, box-shadow 0.2s;">
              <div style="margin-bottom: 12px; font-weight: 600; color: #495057; font-size: 0.95rem; text-transform: uppercase; letter-spacing: 0.5px;">Epoch 10</div>
              <div class="spectrogram-scroll-container" style="border: 2px solid #e9ecef; border-radius: 8px; overflow-x: scroll; overflow-y: hidden; white-space: nowrap; margin-bottom: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); height: 94px; cursor: grab; max-width: 248px;">
                <img src="static/images/baseline_epoch=10_10_soul-groove10_102_4-4_bluebird_v1_noisy.png" alt="Spectrogram Epoch 10" style="height: 90px; width: auto; display: inline-block; max-width: none;">
              </div>
              <audio controls style="width: 100%; height: 36px;">
                <source src="static/audio/baseline/epoch=10_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav" type="audio/wav">
              </audio>
            </div>
            <div style="display: flex; align-items: center; justify-content: center; width: 60px;">
              <span style="font-size: 3rem; background: linear-gradient(135deg, #ff6b6b 0%, #ff8787 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; font-weight: 600; filter: drop-shadow(2px 2px 4px rgba(255, 107, 107, 0.3)); animation: pulse-arrow 2s ease-in-out infinite;">→</span>
            </div>
            <!-- Epoch 20 -->
            <div class="epoch-card" style="background: white; border-radius: 12px; padding: 16px; width: 280px; min-width: 260px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.08); border: 1px solid #f0f0f0; transition: transform 0.2s, box-shadow 0.2s;">
              <div style="margin-bottom: 12px; font-weight: 600; color: #495057; font-size: 0.95rem; text-transform: uppercase; letter-spacing: 0.5px;">Epoch 20</div>
              <div class="spectrogram-scroll-container" style="border: 2px solid #e9ecef; border-radius: 8px; overflow-x: scroll; overflow-y: hidden; white-space: nowrap; margin-bottom: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); height: 94px; cursor: grab; max-width: 248px;">
                <img src="static/images/baseline_epoch=20_10_soul-groove10_102_4-4_bluebird_v1_noisy.png" alt="Spectrogram Epoch 20" style="height: 90px; width: auto; display: inline-block; max-width: none;">
              </div>
              <audio controls style="width: 100%; height: 36px;">
                <source src="static/audio/baseline/epoch=20_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav" type="audio/wav">
              </audio>
            </div>
            <div style="display: flex; align-items: center; justify-content: center; width: 60px;">
              <span style="font-size: 3rem; background: linear-gradient(135deg, #ff6b6b 0%, #ff8787 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; font-weight: 600; filter: drop-shadow(2px 2px 4px rgba(255, 107, 107, 0.3)); animation: pulse-arrow 2s ease-in-out infinite;">→</span>
            </div>
            <!-- Epoch 30 -->
            <div class="epoch-card" style="background: white; border-radius: 12px; padding: 16px; width: 280px; min-width: 260px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.08); border: 1px solid #f0f0f0; transition: transform 0.2s, box-shadow 0.2s;">
              <div style="margin-bottom: 12px; font-weight: 600; color: #495057; font-size: 0.95rem; text-transform: uppercase; letter-spacing: 0.5px;">Epoch 30</div>
              <div class="spectrogram-scroll-container" style="border: 2px solid #e9ecef; border-radius: 8px; overflow-x: scroll; overflow-y: hidden; white-space: nowrap; margin-bottom: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); height: 94px; cursor: grab; max-width: 248px;">
                <img src="static/images/baseline_epoch=30_10_soul-groove10_102_4-4_bluebird_v1_noisy.png" alt="Spectrogram Epoch 30" style="height: 90px; width: auto; display: inline-block; max-width: none;">
              </div>
              <audio controls style="width: 100%; height: 36px;">
                <source src="static/audio/baseline/epoch=30_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav" type="audio/wav">
              </audio>
            </div>
          </div>
        </div>

        <!-- MIDI Conditioned Model Section -->
        <div style="background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%); padding: 2rem; border-radius: 16px; margin-bottom: 2rem; box-shadow: 0 4px 20px rgba(54, 162, 235, 0.1); border: 2px solid #bfdbfe;">
          <h4 class="title is-5" style="margin-bottom: 0rem; color: #1e40af; display: flex; align-items: center; gap: 10px;">
            <span style="background: #3b82f6; width: 8px; height: 24px; border-radius: 4px; display: inline-block;"></span>
            MIDI Conditioned Model
          </h4>
          <div style="display: flex; align-items: center; justify-content: center; gap: 20px; flex-wrap: wrap;">
            <!-- Epoch 0 -->
            <div class="epoch-card" style="background: white; border-radius: 12px; padding: 16px; width: 280px; min-width: 260px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.08); border: 1px solid #f0f0f0; transition: transform 0.2s, box-shadow 0.2s;">
              <div style="margin-bottom: 12px; font-weight: 600; color: #495057; font-size: 0.95rem; text-transform: uppercase; letter-spacing: 0.5px;">Epoch 0</div>
              <div class="spectrogram-scroll-container" style="border: 2px solid #e9ecef; border-radius: 8px; overflow-x: scroll; overflow-y: hidden; white-space: nowrap; margin-bottom: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); height: 94px; cursor: grab; max-width: 248px;">
                <img src="static/images/midi_epoch=0_10_soul-groove10_102_4-4_bluebird_v1_noisy.png" alt="Spectrogram Epoch 0" style="height: 90px; width: auto; display: inline-block; max-width: none;">
              </div>
              <audio controls style="width: 100%; height: 36px;">
                <source src="static/audio/midi_conditioned/epoch=0_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav" type="audio/wav">
              </audio>
            </div>
            <div style="display: flex; align-items: center; justify-content: center; width: 60px;">
              <span style="font-size: 3rem; color: #3b82f6; font-weight: 600; text-shadow: 2px 2px 8px rgba(59, 130, 246, 0.4), 0 0 20px rgba(59, 130, 246, 0.2);">→</span>
            </div>
            <!-- Epoch 10 -->
            <div class="epoch-card" style="background: white; border-radius: 12px; padding: 16px; width: 280px; min-width: 260px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.08); border: 1px solid #f0f0f0; transition: transform 0.2s, box-shadow 0.2s;">
              <div style="margin-bottom: 12px; font-weight: 600; color: #495057; font-size: 0.95rem; text-transform: uppercase; letter-spacing: 0.5px;">Epoch 10</div>
              <div class="spectrogram-scroll-container" style="border: 2px solid #e9ecef; border-radius: 8px; overflow-x: scroll; overflow-y: hidden; white-space: nowrap; margin-bottom: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); height: 94px; cursor: grab; max-width: 248px;">
                <img src="static/images/midi_epoch=10_10_soul-groove10_102_4-4_bluebird_v1_noisy.png" alt="Spectrogram Epoch 10" style="height: 90px; width: auto; display: inline-block; max-width: none;">
              </div>
              <audio controls style="width: 100%; height: 36px;">
                <source src="static/audio/midi_conditioned/epoch=10_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav" type="audio/wav">
              </audio>
            </div>
            <div style="display: flex; align-items: center; justify-content: center; width: 60px;">
              <span style="font-size: 3rem; color: #3b82f6; font-weight: 600; text-shadow: 2px 2px 8px rgba(59, 130, 246, 0.4), 0 0 20px rgba(59, 130, 246, 0.2);">→</span>
            </div>
            <!-- Epoch 20 -->
            <div class="epoch-card" style="background: white; border-radius: 12px; padding: 16px; width: 280px; min-width: 260px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.08); border: 1px solid #f0f0f0; transition: transform 0.2s, box-shadow 0.2s;">
              <div style="margin-bottom: 12px; font-weight: 600; color: #495057; font-size: 0.95rem; text-transform: uppercase; letter-spacing: 0.5px;">Epoch 20</div>
              <div class="spectrogram-scroll-container" style="border: 2px solid #e9ecef; border-radius: 8px; overflow-x: scroll; overflow-y: hidden; white-space: nowrap; margin-bottom: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); height: 94px; cursor: grab; max-width: 248px;">
                <img src="static/images/midi_epoch=20_10_soul-groove10_102_4-4_bluebird_v1_noisy.png" alt="Spectrogram Epoch 20" style="height: 90px; width: auto; display: inline-block; max-width: none;">
              </div>
              <audio controls style="width: 100%; height: 36px;">
                <source src="static/audio/midi_conditioned/epoch=20_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav" type="audio/wav">
              </audio>
            </div>
            <div style="display: flex; align-items: center; justify-content: center; width: 60px;">
              <span style="font-size: 3rem; color: #3b82f6; font-weight: 600; text-shadow: 2px 2px 8px rgba(59, 130, 246, 0.4), 0 0 20px rgba(59, 130, 246, 0.2);">→</span>
            </div>
            <!-- Epoch 30 -->
            <div class="epoch-card" style="background: white; border-radius: 12px; padding: 16px; width: 280px; min-width: 260px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.08); border: 1px solid #f0f0f0; transition: transform 0.2s, box-shadow 0.2s;">
              <div style="margin-bottom: 12px; font-weight: 600; color: #495057; font-size: 0.95rem; text-transform: uppercase; letter-spacing: 0.5px;">Epoch 30</div>
              <div class="spectrogram-scroll-container" style="border: 2px solid #e9ecef; border-radius: 8px; overflow-x: scroll; overflow-y: hidden; white-space: nowrap; margin-bottom: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); height: 94px; cursor: grab; max-width: 248px;">
                <img src="static/images/midi_epoch=30_10_soul-groove10_102_4-4_bluebird_v1_noisy.png" alt="Spectrogram Epoch 30" style="height: 90px; width: auto; display: inline-block; max-width: none;">
              </div>
              <audio controls style="width: 100%; height: 36px;">
                <source src="static/audio/midi_conditioned/epoch=30_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav" type="audio/wav">
              </audio>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>
            We presented a MIDI-conditioned diffusion model for drum audio enhancement that extends SGMSE with Feature-wise Linear Modulation (FiLM) layers. By incorporating symbolic musical information through a MIDI encoder, our model makes musically-informed enhancement decisions that preserve rhythmic structure and drum timbre. The results demonstrate that the CTC loss-enhanced model achieves consistently lower Frechet VGGish Scores compared to the baseline, with perceptual evaluation confirming superior denoising quality. This work validates the effectiveness of MIDI conditioning for audio enhancement and opens promising directions for extending this approach to other instruments, multi-track scenarios, and real-time applications.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="#" class="external-link" disabled>
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="#" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            <strong>How to Cite:</strong><br>
            Aspis, E., & Gannot, S. (2025). <em>MiDiff: MIDI-Conditioned Diffusion Models for Drum Audio Enhancement</em>.
          </p>
          <p>
            &copy; 2025 Elad Aspis, Bar-Ilan University. All Rights Reserved.
          </p>
          <p>
            <strong>Credits:</strong><br>
            For the website we used the source code from the <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies</a> website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

<script>
// Initialize SeeWav instances for all audio files
document.addEventListener('DOMContentLoaded', function() {
  
  // Configuration for SeeWav waveforms
  const waveConfig = {
    height: 60,
    width: '100%',
    color: '#3273dc',
    progressColor: '#209cee',
    backgroundColor: '#f5f5f5',
    cursorColor: '#363636',
    responsive: true,
    normalize: true,
    pixelRatio: window.devicePixelRatio || 1
  };

  // Function to create SeeWav instance
  function createSeeWav(containerId, audioSrc) {
    const container = document.getElementById(containerId);
    if (!container) return null;
    
    return new SeeWav({
      container: container,
      url: audioSrc,
      ...waveConfig
    });
  }

  // Reference Audio Waveforms
  const cleanWave = createSeeWav('waveform-clean', 'static/clean_10_soul-groove10_102_4-4_bluebird.wav');
  const noisyWave = createSeeWav('waveform-noisy', 'static/noisy_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav');

  // Model Outputs Waveforms - 0 Epochs (Initial)
  const baseline0Wave = createSeeWav('waveform-baseline-0', 'static/baseline_model/epoch0_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav');
  const ctc0Wave = createSeeWav('waveform-ctc-0', 'static/ctc_loss/epoch0_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav');

  // Model Outputs Waveforms - 10 Epochs
  const baseline10Wave = createSeeWav('waveform-baseline-10', 'static/baseline_model/epoch10_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav');
  const ctc10Wave = createSeeWav('waveform-ctc-10', 'static/ctc_loss/epoch10_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav');

  // Model Outputs Waveforms - 20 Epochs
  const baseline20Wave = createSeeWav('waveform-baseline-20', 'static/baseline_model/epoch20_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav');
  const ctc20Wave = createSeeWav('waveform-ctc-20', 'static/ctc_loss/epoch20_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav');

  // Sync waveforms with audio controls
  const audioMappings = [
    { audioId: 'audio-clean', wave: cleanWave },
    { audioId: 'audio-noisy', wave: noisyWave },
    { audioId: 'audio-baseline-0', wave: baseline0Wave },
    { audioId: 'audio-ctc-0', wave: ctc0Wave },
    { audioId: 'audio-baseline-10', wave: baseline10Wave },
    { audioId: 'audio-ctc-10', wave: ctc10Wave },
    { audioId: 'audio-baseline-20', wave: baseline20Wave },
    { audioId: 'audio-ctc-20', wave: ctc20Wave }
  ];

  audioMappings.forEach(({ audioId, wave }) => {
    if (!wave) return;
    
    const audioEl = document.getElementById(audioId);
    if (!audioEl) return;
    
    // Sync audio controls with waveform
    audioEl.addEventListener('play', () => {
      if (wave.play) wave.play();
    });
    
    audioEl.addEventListener('pause', () => {
      if (wave.pause) wave.pause();
    });
    
    audioEl.addEventListener('timeupdate', () => {
      if (wave.seekTo && audioEl.duration) {
        const progress = audioEl.currentTime / audioEl.duration;
        wave.seekTo(progress);
      }
    });

    // Click on waveform to control audio
    if (wave.on) {
      wave.on('click', (progress) => {
        if (audioEl.duration) {
          audioEl.currentTime = progress * audioEl.duration;
        }
      });
      
      wave.on('play', () => {
        audioEl.play();
      });
      
      wave.on('pause', () => {
        audioEl.pause();
      });
    }
  });

  // Alternative approach if SeeWav doesn't have event listeners
  // Add click handlers to waveform containers
  document.querySelectorAll('[id^="waveform-"]').forEach(container => {
    container.addEventListener('click', (e) => {
      const rect = container.getBoundingClientRect();
      const clickX = e.clientX - rect.left;
      const progress = clickX / rect.width;
      
      // Find corresponding audio element
      const waveformId = container.id;
      const audioId = waveformId.replace('waveform-', 'audio-');
      const audioEl = document.getElementById(audioId);
      
      if (audioEl && audioEl.duration) {
        audioEl.currentTime = progress * audioEl.duration;
      }
    });
  });
});
</script>


<script>
// Spectrogram functionality removed - keeping only audio players
console.log('Audio players initialized without spectrogram visualization');
</script>

<script>
// Function to load CSV data
async function loadCSV(url) {
  const response = await fetch(url);
  const text = await response.text();
  const lines = text.trim().split('\n');
  const headers = lines[0].split(',');
  const data = [];
  
  for (let i = 1; i < lines.length; i++) {
    const values = lines[i].split(',');
    const row = {};
    headers.forEach((header, index) => {
      row[header.trim()] = parseFloat(values[index]) || values[index];
    });
    data.push(row);
  }
  return data;
}

// Initialize the chart
async function initChart() {
    const N = frameData.length;
    const magnitudes = new Float32Array(N / 2);
    
    // Ultra-fast DFT - only calculate every 2nd frequency bin and interpolate
    for (let k = 0; k < N / 2; k += 2) {
      let real = 0;
      let imag = 0;
      
      // Sample every 8th point for maximum speed
      for (let n = 0; n < N; n += 8) {
        const angle = -2 * Math.PI * k * n / N;
        real += frameData[n] * Math.cos(angle);
        imag += frameData[n] * Math.sin(angle);
      }
      
      const magnitude = Math.sqrt(real * real + imag * imag);
      magnitudes[k] = magnitude;
      
      // Simple interpolation for skipped bin
      if (k + 1 < N / 2) {
        magnitudes[k + 1] = magnitude * 0.8; // Approximate
      }
    }
    
    return magnitudes;
  }

  // Adobe Audition color mapping (optimized)
  getAdobeAuditionColor(dbValue) {
    const minDb = -80;
    const maxDb = -10;
    const normalized = Math.max(0, Math.min(1, (dbValue - minDb) / (maxDb - minDb)));
    
    let r, g, b;
    
    if (normalized <= 0.05) {
      // Almost black for very low values
      r = g = b = Math.floor(normalized * 200);
    } else if (normalized < 0.2) {
      // Dark blue
      const t = (normalized - 0.05) / 0.15;
      r = 0;
      g = Math.floor(t * 30);
      b = Math.floor(60 + t * 140);
    } else if (normalized < 0.4) {
      // Blue to purple
      const t = (normalized - 0.2) / 0.2;
      r = Math.floor(t * 120);
      g = Math.floor(30 + t * 20);
      b = Math.floor(200 + t * 55);
    } else if (normalized < 0.6) {
      // Purple to magenta
      const t = (normalized - 0.4) / 0.2;
      r = Math.floor(120 + t * 135);
      g = Math.floor(50 - t * 30);
      b = Math.floor(255 - t * 100);
    } else if (normalized < 0.8) {
      // Magenta to red/orange
      const t = (normalized - 0.6) / 0.2;
      r = 255;
      g = Math.floor(20 + t * 100);
      b = Math.floor(155 - t * 155);
    } else {
      // Orange to yellow
      const t = (normalized - 0.8) / 0.2;
      r = 255;
      g = Math.floor(120 + t * 135);
      b = Math.floor(t * 80);
    }
    
    return [Math.min(255, r), Math.min(255, g), Math.min(255, b)];
  }

  // Optimized spectrogram generation
  async generateSpectrogram(audioBuffer, canvasId) {
    try {
      const canvas = document.getElementById(canvasId);
      if (!canvas) {
        console.error(`Canvas ${canvasId} not found`);
        return;
      }

      const ctx = canvas.getContext('2d');
      const channelData = audioBuffer.getChannelData(0);
      const sampleRate = audioBuffer.sampleRate;
      
      console.log(`Generating clean spectrogram for ${canvasId}`);
      console.log(`Audio: ${channelData.length} samples, ${sampleRate}Hz`);
      
      // Set fixed canvas dimensions to prevent scaling issues
      const width = 800;
      const height = 300;
      
      // Set canvas internal dimensions exactly
      canvas.width = width;
      canvas.height = height;
      
      // Set CSS to match exactly - no scaling
      canvas.style.width = `${width}px`;
      canvas.style.height = `${height}px`;
      canvas.style.maxWidth = 'none';
      canvas.style.display = 'block';
      canvas.style.imageRendering = 'pixelated';
      
      // Clear the canvas completely first
      ctx.fillStyle = '#000000';
      ctx.fillRect(0, 0, width, height);
      
      // Update internal dimensions for rendering
      this.scrollViewWidth = width;
      this.scrollViewHeight = height;

      // Calculate high-resolution parameters
      const framesPerSecond = sampleRate / this.hopSize;
      const totalFrames = Math.floor((channelData.length - this.fftSize) / this.hopSize) + 1;
      const totalDuration = totalFrames / framesPerSecond;
      const frequencyBins = this.fftSize / 2;
      
      console.log(`🚀 Clean rendering mode: ${totalFrames} frames, ${totalDuration.toFixed(1)}s total`);
      console.log(`📊 Processing ${this.initialComputeSeconds}s for immediate display`);
      
      // Pre-create window
      const window = this.createHanningWindow(this.fftSize);
      
      // Calculate initial processing range (first 5 seconds)
      const initialFrames = Math.min(totalFrames, Math.floor(this.initialComputeSeconds * framesPerSecond));
      const spectrogramData = new Array(totalFrames); // Pre-allocate full array
      
      console.log(`⚡ CLEAN MODE: Processing ${initialFrames} frames for crisp display`);
      
      // Process initial frames with maximum speed
      const batchSize = 25; // Smaller batches for responsiveness
      for (let batchStart = 0; batchStart < initialFrames; batchStart += batchSize) {
        const batchEnd = Math.min(batchStart + batchSize, initialFrames);
        
        // Clean progress display without overlays
        if (batchStart % 200 === 0) {
          const progress = Math.floor((batchStart / initialFrames) * 100);
          console.log(`Processing: ${progress}% complete`);
        }
        
        
        // Process batch with minimal overhead
        for (let frameIndex = batchStart; frameIndex < batchEnd; frameIndex++) {
          const startSample = frameIndex * this.hopSize;

          // Extract windowed frame (simplified)
          const frameData = new Float32Array(this.fftSize);
          const endSample = Math.min(startSample + this.fftSize, channelData.length);
          
          for (let i = 0; i < this.fftSize && startSample + i < channelData.length; i++) {
            frameData[i] = channelData[startSample + i] * window[i];
          }
          
          // Calculate magnitude spectrum
          const magnitudes = this.calculateMagnitudeSpectrum(frameData);
          spectrogramData[frameIndex] = magnitudes;
        }
        
        // No delays for maximum speed
      }

      console.log(`🚀 CLEAN RENDER READY! ${this.initialComputeSeconds}s computed for display`);      
      
      // Store canvas info
      const spectrogramFramesPerSecond = sampleRate / this.hopSize;
      this.storeCanvasInfo(canvasId, width, height, totalDuration, spectrogramData, spectrogramFramesPerSecond, frequencyBins, sampleRate, channelData, window, initialFrames);
      
      // Single clean render
      this.renderScrollingView(canvasId, 0);
      
    } catch (error) {
      console.error(`Error generating spectrogram for ${canvasId}:`, error);
      throw error;
    }
  }

  // Ultra-fast streaming computation (minimal overhead)
  async streamingCompute(canvasId, targetTime) {
    const info = this.canvasInfo[canvasId];
    if (!info || !info.channelData) return;
    
    const targetFrame = Math.floor(targetTime * info.framesPerSecond);
    const computeStartFrame = info.computedFrames || 0;
    
    // Check if we need to compute more frames (smaller buffer for speed)
    if (targetFrame <= computeStartFrame + 50) return; // Reduced buffer
    
    const computeEndFrame = Math.min(
      info.spectrogramData.length - 1,
      targetFrame + Math.floor(this.streamingBatchSeconds * info.framesPerSecond)
    );
    
    if (computeEndFrame <= computeStartFrame) return;
    
    // Process in ultra-fast background mode
    const processFrames = async () => {
      const batchSize = 10; // Even smaller batches
      
      for (let frameIndex = computeStartFrame; frameIndex <= computeEndFrame; frameIndex += batchSize) {
        const batchEnd = Math.min(frameIndex + batchSize, computeEndFrame);
        
        for (let i = frameIndex; i <= batchEnd; i++) {
          if (i < info.spectrogramData.length && !info.spectrogramData[i]) {
            const startSample = i * this.hopSize;
            
            // Ultra-fast frame extraction
            const frameData = new Float32Array(this.fftSize);
            for (let j = 0; j < this.fftSize && startSample + j < info.channelData.length; j++) {
              frameData[j] = info.channelData[startSample + j] * info.window[j];
            }
            
            // Calculate magnitude spectrum
            const magnitudes = this.calculateMagnitudeSpectrum(frameData);
            info.spectrogramData[i] = magnitudes;
          }
        }
        
        // Minimal delay
        await new Promise(resolve => setTimeout(resolve, 1));
      }
      
      info.computedFrames = computeEndFrame;
    };
    
    // Run in background without blocking
    processFrames().catch(console.error);
  }

  // Store canvas information for streaming spectrogram
  storeCanvasInfo(canvasId, width, height, duration, spectrogramData, framesPerSecond, frequencyBins, sampleRate, channelData, window, computedFrames) {
    if (!this.canvasInfo) {
      this.canvasInfo = {};
    }
    this.canvasInfo[canvasId] = {
      width: width,
      height: height,
      duration: duration,
      spectrogramData: spectrogramData, // Sparse array - only computed portions filled
      framesPerSecond: framesPerSecond,
      frequencyBins: frequencyBins,
      sampleRate: sampleRate,
      currentScrollTime: 0,
      // Streaming data
      channelData: channelData,
      window: window,
      computedFrames: computedFrames
    };
    this.isStreaming.set(canvasId, false);
  }

  // Render scrolling view of spectrogram
  renderScrollingView(canvasId, centerTime) {
    if (!this.canvasInfo || !this.canvasInfo[canvasId]) return;
    
    const canvas = document.getElementById(canvasId);
    if (!canvas) return;
    
    const ctx = canvas.getContext('2d');
    const info = this.canvasInfo[canvasId];
    
    // Calculate scrolling time window
    const windowDuration = Math.min(this.timeWindowSeconds, info.duration);
    const startTime = Math.max(0, centerTime - windowDuration / 2);
    const endTime = Math.min(info.duration, centerTime + windowDuration / 2);
    
    // Calculate frame indices for visible window
    const startFrame = Math.floor(startTime * info.framesPerSecond);
    const endFrame = Math.min(info.spectrogramData.length - 1, Math.floor(endTime * info.framesPerSecond));
    const visibleFrames = Math.max(1, endFrame - startFrame + 1);
    
    // Clear canvas completely first
    ctx.clearRect(0, 0, info.width, info.height);
    ctx.fillStyle = '#000000';
    ctx.fillRect(0, 0, info.width, info.height);
    
    if (visibleFrames <= 0) return;
    
    console.log(`Rendering: ${info.width}x${info.height} canvas, ${visibleFrames} frames, ${info.frequencyBins} freq bins`);
    
    // Find dynamic range for this time window (only from computed frames)
    let minMagnitude = Infinity;
    let maxMagnitude = -Infinity;
    
    for (let frameIdx = startFrame; frameIdx <= endFrame; frameIdx++) {
      if (frameIdx >= 0 && frameIdx < info.spectrogramData.length && info.spectrogramData[frameIdx]) {
        const frame = info.spectrogramData[frameIdx];
        for (const mag of frame) {
          if (mag > 0) {
            const db = 20 * Math.log10(Math.max(mag, 1e-10));
            minMagnitude = Math.min(minMagnitude, db);
            maxMagnitude = Math.max(maxMagnitude, db);
          }
        }
      }
    }
    
    // Set default range if no data computed yet
    if (minMagnitude === Infinity) {
      minMagnitude = -80;
      maxMagnitude = -10;
    }
    
    // Create single clean imageData render
    const imageData = ctx.createImageData(info.width, info.height);
    const data = imageData.data;
    
    // Render scrolling window of spectrogram
    for (let x = 0; x < info.width; x++) {
      const timeProgress = x / info.width;
      const currentTime = startTime + timeProgress * (endTime - startTime);
      const currentFrameIdx = Math.floor(currentTime * info.framesPerSecond);
      
      if (currentFrameIdx >= 0 && currentFrameIdx < info.spectrogramData.length && info.spectrogramData[currentFrameIdx]) {
        const frame = info.spectrogramData[currentFrameIdx];
        
        for (let y = 0; y < info.height; y++) {
          const freqBin = Math.floor(((info.height - 1 - y) / info.height) * info.frequencyBins);
          if (freqBin < frame.length) {
            const magnitude = frame[freqBin];
            const db = magnitude > 0 ? 20 * Math.log10(Math.max(magnitude, 1e-10)) : minMagnitude;
            
            const color = this.getAdobeAuditionColor(db);
            
            const pixelIndex = (y * info.width + x) * 4;
            data[pixelIndex] = color[0];
            data[pixelIndex + 1] = color[1];
            data[pixelIndex + 2] = color[2];
            data[pixelIndex + 3] = 255;
          }
        }
      }
      // Black for uncomputed regions
    }
    
    // Single render to canvas - no overlays to prevent artifacts
    ctx.putImageData(imageData, 0, 0);
    
    // Skip grid and labels to prevent visual artifacts
    // Clean spectrogram display only
    
    // Update stored scroll time
    info.currentScrollTime = centerTime;
  }

  // Draw time grid and labels
  drawTimeGrid(ctx, info, startTime, endTime) {
    const timeRange = endTime - startTime;
    const timeStep = timeRange / 10; // 10 time divisions
    
    ctx.strokeStyle = 'rgba(255, 255, 255, 0.2)';
    ctx.lineWidth = 1;
    ctx.font = '10px Arial';
    ctx.fillStyle = 'white';
    
    for (let i = 0; i <= 10; i++) {
      const time = startTime + i * timeStep;
      const x = (i / 10) * info.width;
      
      // Draw vertical grid line
      ctx.beginPath();
      ctx.moveTo(x, 0);
      ctx.lineTo(x, info.height);
      ctx.stroke();
      
      // Draw time label
      const timeLabel = time.toFixed(1) + 's';
      ctx.fillStyle = 'rgba(0, 0, 0, 0.8)';
      ctx.fillRect(x - 15, info.height - 16, 30, 14);
      ctx.fillStyle = 'white';
      ctx.fillText(timeLabel, x - 12, info.height - 6);
    }
  }

  // Draw frequency labels
  drawFrequencyLabels(ctx, info) {
    const nyquist = info.sampleRate / 2;
    const freqLabels = [
      { freq: 100, label: '100Hz' },
      { freq: 1000, label: '1kHz' },
      { freq: 2000, label: '2kHz' },
      { freq: 5000, label: '5kHz' },
      { freq: 10000, label: '10kHz' },
      { freq: 16000, label: '16kHz' }
    ];
    
    ctx.font = '10px Arial';
    ctx.strokeStyle = 'black';
    ctx.lineWidth = 1;
    
    freqLabels.forEach(({ freq, label }) => {
      if (freq <= nyquist) {
        const y = info.height - (freq / nyquist) * info.height;
        if (y > 15 && y < info.height - 5) {
          // Background for readability
          ctx.fillStyle = 'rgba(0, 0, 0, 0.8)';
          ctx.fillRect(2, y - 8, 35, 12);
          
          // Text outline
          ctx.strokeText(label, 5, y);
          
          // Text fill
          ctx.fillStyle = 'white';
          ctx.fillText(label, 5, y);
        }
      }
    });
  }

  // Clean time cursor with minimal overlay
  drawTimeCursor(canvasId, currentTime) {
    if (!this.canvasInfo || !this.canvasInfo[canvasId]) return;
    
    const canvas = document.getElementById(canvasId);
    if (!canvas) return;
    
    const ctx = canvas.getContext('2d');
    const info = this.canvasInfo[canvasId];
    
    // Re-render spectrogram first
    this.renderScrollingView(canvasId, currentTime);
    
    // Draw simple time cursor line
    const progress = currentTime / info.duration;
    const cursorX = progress * info.width;
    
    ctx.strokeStyle = '#00ff88';
    ctx.lineWidth = 2;
    ctx.beginPath();
    ctx.moveTo(cursorX, 0);
    ctx.lineTo(cursorX, info.height);
    ctx.stroke();
  }

  // Setup audio sync for a specific audio element and canvas
  setupAudioSync(audioId, canvasId) {
    const audio = document.getElementById(audioId);
    if (!audio) {
      console.warn(`Audio element ${audioId} not found for sync`);
      return;
    }
    
    // Update time cursor during playback
    const updateCursor = () => {
      if (!audio.paused) {
        this.drawTimeCursor(canvasId, audio.currentTime);
        requestAnimationFrame(updateCursor);
      }
    };
    
    // Start cursor update when audio plays
    audio.addEventListener('play', () => {
      updateCursor();
    });
    
    // Update cursor when seeking
    audio.addEventListener('timeupdate', () => {
      if (audio.paused) {
        this.drawTimeCursor(canvasId, audio.currentTime);
      }
    });
    
    // Clear cursor when audio ends or pauses
    audio.addEventListener('pause', () => {
      this.drawTimeCursor(canvasId, audio.currentTime);
    });
    
    audio.addEventListener('ended', () => {
      this.drawTimeCursor(canvasId, 0);
    });
    
    console.log(`✅ Audio sync setup for ${audioId} -> ${canvasId}`);
  }

  // Load and process audio
  async loadAndGenerateSpectrogram(audioUrl, canvasId) {
    try {
      console.log(`🎵 Loading audio for optimized spectrogram: ${audioUrl}`);
      
      // Simple loading indication
      const canvas = document.getElementById(canvasId);
      if (canvas) {
        console.log(`🎵 Loading ${canvasId}...`);
      }
      
      const response = await fetch(audioUrl);
      if (!response.ok) {
        throw new Error(`Failed to load: ${response.status}`);
      }
      
      const arrayBuffer = await response.arrayBuffer();
      console.log(`Audio loaded: ${(arrayBuffer.byteLength / 1024).toFixed(0)}KB`);
      
      const audioContext = new (window.AudioContext || window.webkitAudioContext)();
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      
      console.log(`Audio ready: ${audioBuffer.duration.toFixed(1)}s @ ${audioBuffer.sampleRate}Hz`);
      
      await this.generateSpectrogram(audioBuffer, canvasId);
      
    } catch (error) {
      console.error(`❌ Error loading ${canvasId}:`, error);
      
      const canvas = document.getElementById(canvasId);
      if (canvas) {
        const ctx = canvas.getContext('2d');
        canvas.width = 800;
        canvas.height = 256;
        ctx.fillStyle = '#330000';
        ctx.fillRect(0, 0, 800, 256);
        ctx.fillStyle = '#ff6666';
        ctx.font = '12px Arial';
        ctx.fillText(`Error: ${error.message}`, 10, 50);
      }
    }
  }
}

// Simple test visualization - draw immediately
function drawTestPattern(canvasId, color) {
  const canvas = document.getElementById(canvasId);
  if (!canvas) {
    console.error(`Canvas ${canvasId} not found!`);
    return;
  }
  
  console.log(`Drawing test pattern for ${canvasId}`);
  const ctx = canvas.getContext('2d');
  
  // Get the display size
  const rect = canvas.getBoundingClientRect();
  canvas.width = rect.width || 400;
  canvas.height = 120;
  
  // Draw background
  ctx.fillStyle = '#1a1a2e';
  ctx.fillRect(0, 0, canvas.width, canvas.height);
  
  // Draw some bars to test
  ctx.fillStyle = color;
  for (let i = 0; i < 50; i++) {
    const x = (i / 50) * canvas.width;
    const barHeight = Math.random() * canvas.height;
    ctx.fillRect(x, canvas.height - barHeight, canvas.width / 50 - 2, barHeight);
  }
  
  // Draw text
  ctx.fillStyle = '#ffffff';
  ctx.font = '16px Arial';
  ctx.fillText('Audio Visualization', 10, 30);
  
  console.log(`✓ Test pattern drawn for ${canvasId}: ${canvas.width}x${canvas.height}`);
}

// Initialize spectrograms with audio sync
document.addEventListener('DOMContentLoaded', function() {
  console.log('🎵 Initializing spectrograms with audio sync...');
  
  const spectrogramGenerator = new SpectrogramGenerator();
  
  // Draw test patterns immediately
  console.log('� Drawing test visualizations...');
  drawTestPattern('spec-clean', '#00ff88');
  drawTestPattern('spec-noisy', '#ff9900');
  
  setTimeout(() => {
    console.log('🚀 Starting spectrogram generation with audio sync...');
    
    // Generate main spectrograms with audio sync
    const canvasesToGenerate = [
      { id: 'spectrogram-clean', url: 'static/clean_10_soul-groove10_102_4-4_bluebird.wav', audio: 'audio-clean' },
      { id: 'spectrogram-noisy', url: 'static/noisy_10_soul-groove10_102_4-4_bluebird_v1_noisy.wav', audio: 'audio-noisy' }
    ];
    
    // Generate spectrograms with audio synchronization
    canvasesToGenerate.forEach((item, index) => {
      const canvas = document.getElementById(item.id);
      if (canvas) {
        console.log(`✅ Found canvas: ${item.id}`);
        setTimeout(() => {
          spectrogramGenerator.loadAndGenerateSpectrogram(item.url, item.id);
          
          // Setup audio sync after generation
          setTimeout(() => {
            spectrogramGenerator.setupAudioSync(item.audio, item.id);
          }, 2000);
        }, index * 1500); // Staggered generation
      } else {
        console.log(`⚠️ Canvas not found: ${item.id}`);
      }
    });
    
  }, 1000);
});
</script>

<script>
// Function to load CSV data
async function loadCSV(url) {
  const response = await fetch(url);
  const text = await response.text();
  const lines = text.trim().split('\n');
  const headers = lines[0].split(',');
  const data = [];
  
  for (let i = 1; i < lines.length; i++) {
    const values = lines[i].split(',');
    const row = {};
    headers.forEach((header, index) => {
      row[header.trim()] = parseFloat(values[index]) || values[index];
    });
    data.push(row);
  }
  return data;
}

// Initialize the chart
async function initChart() {
  console.log('Starting chart initialization...');
  
  try {
    console.log('Loading CSV files...');
    // Load both CSV files
    const baselineData = await loadCSV('static/graphs/frechet_loss/tensorboard_logs_version_22.csv');
    const ctcData = await loadCSV('static/graphs/frechet_loss/tensorboard_logs_version_20.csv');
    
    console.log('Baseline data:', baselineData.length, 'points');
    console.log('CTC data:', ctcData.length, 'points');
    console.log('Sample baseline data:', baselineData.slice(0, 3));
    console.log('Sample CTC data:', ctcData.slice(0, 3));

    const chartElement = document.getElementById('frechetChart');
    console.log('Chart element found:', chartElement);
    
    if (!chartElement) {
      console.error('Chart element not found!');
      return;
    }

    const ctx = chartElement.getContext('2d');
    console.log('Canvas context:', ctx);
    
    window.frechetChart = new Chart(ctx, {
      type: 'line',
      data: {
        datasets: [{
          label: 'Baseline Model',
          data: baselineData.map(row => ({
            x: row.Step,
            y: row.Value
          })),
          borderColor: 'rgb(255, 99, 132)',
          backgroundColor: 'rgba(255, 99, 132, 0.2)',
          borderWidth: 2,
          pointRadius: 4,
          pointHoverRadius: 6
        }, {
          label: 'Midi Conditioned Model',
          data: ctcData.map(row => ({
            x: row.Step,
            y: row.Value
          })),
          borderColor: 'rgb(54, 162, 235)',
          backgroundColor: 'rgba(54, 162, 235, 0.2)',
          borderWidth: 2,
          pointRadius: 4,
          pointHoverRadius: 6
        }]
      },
      options: {
        responsive: true,
        maintainAspectRatio: false,
        scales: {
          x: {
            type: 'linear',
            title: {
              display: true,
              text: 'Training Steps'
            }
          },
          y: {
            type: 'logarithmic',
            title: {
              display: true,
              text: 'Frechet VGGish Score'
            }
          }
        },
        plugins: {
          title: {
            display: true,
            text: 'Frechet VGGish Score Comparison During Training'
          },
          legend: {
            display: true,
            position: 'top'
          },
          tooltip: {
            mode: 'point',
            intersect: false,
            callbacks: {
              label: function(context) {
                return `${context.dataset.label}: ${context.parsed.y.toFixed(4)} (Step: ${context.parsed.x})`;
              }
            }
          }
        },
        interaction: {
          mode: 'nearest',
          axis: 'x',
          intersect: false
        }
      }
    });
    
    console.log('Chart created successfully:', window.frechetChart);
  } catch (error) {
    console.error('Error loading chart data:', error);
    document.getElementById('frechetChart').parentElement.innerHTML = 
      '<p style="text-align: center; color: red;">Error loading chart data: ' + error.message + '</p>';
  }
}

// Toggle logarithmic scale
function toggleLogScale() {
  const checkbox = document.getElementById('logScaleToggle');
  if (window.frechetChart) {
    window.frechetChart.options.scales.y.type = checkbox.checked ? 'logarithmic' : 'linear';
    window.frechetChart.update();
  }
}

// Initialize chart when page loads
document.addEventListener('DOMContentLoaded', initChart);
</script>

<style>
  /* Arrow pulse animation */
  @keyframes pulse-arrow {
    0%, 100% {
      transform: scale(1);
      opacity: 0.9;
    }
    50% {
      transform: scale(1.1);
      opacity: 1;
    }
  }

  /* MIDI Player Consistent Styling */
  midi-player {
    width: 100% !important;
    height: 40px !important;
    max-height: 40px !important;
    min-height: 40px !important;
    border-radius: 8px !important;
    display: block !important;
    margin: 0 !important;
    padding: 0 !important;
    box-sizing: border-box !important;
    overflow: hidden !important;
    font-size: 12px !important;
  }
  
  /* Target internal elements of midi-player */
  midi-player::part(control-panel) {
    height: 40px !important;
    max-height: 40px !important;
  }

  /* Force MIDI player field and control to have no extra spacing */
  .box .field .control midi-player {
    line-height: 40px !important;
  }
  
  .box .field .control {
    padding: 0 !important;
    margin: 0 !important;
  }

  /* Remove extra spacing from Bulma's field class in audio reference boxes */
  .box .field:last-child {
    margin-bottom: 0 !important;
  }

  /* Make all three reference cards equal height */
  .columns.is-centered > .column.is-4 {
    display: flex !important;
  }
  
  .columns.is-centered > .column.is-4 > .box {
    display: flex !important;
    flex-direction: column !important;
    width: 100% !important;
    min-height: 100% !important;
  }

  /* Responsive Design Adjustments */
  @media (max-width: 768px) {
    .container {
      padding: 0 15px;
    }
    
    .title.is-1 {
      font-size: 2.5rem;
    }
    
    .title.is-2 {
      font-size: 2rem;
    }
    
    .title.is-3 {
      font-size: 1.75rem;
    }
    
    .table-container {
      overflow-x: auto;
    }
    
    .columns {
      flex-direction: column;
    }
    
    .column {
      width: 100%;
    }
    
    #chart-container {
      height: 300px;
    }
  }
</style>

<script>
// Drag-to-scroll functionality for spectrogram containers
document.addEventListener('DOMContentLoaded', function() {
  const spectrogramContainers = document.querySelectorAll('.spectrogram-scroll-container');
  
  spectrogramContainers.forEach(container => {
    let isDown = false;
    let startX;
    let scrollLeft;
    
    container.addEventListener('mousedown', (e) => {
      isDown = true;
      container.style.cursor = 'grabbing';
      startX = e.pageX - container.offsetLeft;
      scrollLeft = container.scrollLeft;
      e.preventDefault();
    });
    
    container.addEventListener('mouseleave', () => {
      isDown = false;
      container.style.cursor = 'grab';
    });
    
    container.addEventListener('mouseup', () => {
      isDown = false;
      container.style.cursor = 'grab';
    });
    
    container.addEventListener('mousemove', (e) => {
      if (!isDown) return;
      e.preventDefault();
      const x = e.pageX - container.offsetLeft;
      const walk = (x - startX) * 2; // Scroll speed multiplier
      container.scrollLeft = scrollLeft - walk;
    });
  });
});

// Force MathJax to typeset when page loads
document.addEventListener('DOMContentLoaded', function() {
  if (window.MathJax) {
    MathJax.typesetPromise().then(() => {
      console.log('MathJax typesetting complete');
    }).catch((err) => console.log('MathJax typesetting failed: ', err));
  } else {
    // Wait for MathJax to load
    setTimeout(() => {
      if (window.MathJax) {
        MathJax.typesetPromise().then(() => {
          console.log('MathJax typesetting complete (delayed)');
        }).catch((err) => console.log('MathJax typesetting failed: ', err));
      }
    }, 2000);
  }
});
</script>

  </body>
  </html>
